{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lineare klassifikation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_in(X, y, w):\n",
    "    return np.sum(H(X, w) != y) / X.shape[0]\n",
    "\n",
    "# phi (x -> {0, 1})\n",
    "def phi(x):\n",
    "    return np.sign(x)\n",
    "\n",
    "# perzeptron (x, w -> y)\n",
    "def h(x, w):\n",
    "    return phi(w @ x)\n",
    "\n",
    "def H(X, w):\n",
    "    return [h(x, w) for x in X]\n",
    "\n",
    "# p(x) (x, w -> gerade)\n",
    "def p(x, w):\n",
    "    return -w[1]/w[2]*x - w[0]/w[2]\n",
    "\n",
    "def P(X, w):\n",
    "    return [p(x, w) for x in X]\n",
    "\n",
    "# pla (X, y -> w)\n",
    "def pla(X, y, w=None, t=0):\n",
    "    if t == 0:\n",
    "        X = np.column_stack((np.ones(X.shape[0]), X))\n",
    "    if w is None:\n",
    "        w = np.zeros(X.shape[1])\n",
    "\n",
    "    for i, x in enumerate(X):\n",
    "        if h(x, w) != y[i]:\n",
    "            return pla(X, y, w + y[i]*x, t+1)\n",
    "\n",
    "    return w\n",
    "\n",
    "# pocket (X, y -> w)\n",
    "def pocket(X, y, T):\n",
    "    X = np.column_stack((np.ones(X.shape[0]), X))\n",
    "\n",
    "    w = np.zeros(X.shape[1])\n",
    "    w_d = w.copy()\n",
    "    e = 1\n",
    "\n",
    "    for _ in range(T):\n",
    "        for i, x in enumerate(X):\n",
    "            if h(x, w) != y[i]:\n",
    "                w += y[i]*x\n",
    "\n",
    "                if e_in(X, y, w) < e:\n",
    "                    w_d = w.copy()\n",
    "                    e = e_in(X, y, w_d)\n",
    "\n",
    "    return w_d\n",
    "\n",
    "def scatter(X, y, w):\n",
    "    xlim = (X[:,0].min()-0.1, X[:,0].max()+0.1)\n",
    "    ylim = (X[:,1].min()-0.1, X[:,1].max()+0.1)\n",
    "\n",
    "    plt.scatter(X[:,0], X[:,1], c=y)\n",
    "    plt.plot(xlim, P(xlim, w))\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lineare regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_in(X, y, w, Q=4):\n",
    "    g = G(X, w, Q)\n",
    "\n",
    "    return float((g-y).T@(g-y) / X.shape[0])\n",
    "\n",
    "def e_out(f, w, start=-1, stop=1, K=50):\n",
    "    L = np.linspace(start, stop, K)\n",
    "    \n",
    "    return e_in(L, f(L), w)\n",
    "    \n",
    "# g ~ f (x, w -> y)\n",
    "def g(x, w):\n",
    "    return np.hstack((np.ones(1), x)) @ w\n",
    "\n",
    "def G(X, w):\n",
    "    return [g(x, w) for x in X]\n",
    "\n",
    "# lineare regression (X, y -> w)\n",
    "def lin_reg(X, y):\n",
    "    X = np.column_stack((np.ones(X.shape[0]), X))\n",
    "\n",
    "    X_d = np.linalg.inv(X.T @ X) @ X.T\n",
    "    return X_d @ y\n",
    "\n",
    "def scatter(X, y, w):\n",
    "    xlim = (X.min()-0.1, X.max()+0.1)\n",
    "    ylim = (y.min()-0.1, y.max()+0.1)\n",
    "\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"y\")\n",
    "\n",
    "    plt.legend(handles=[\n",
    "        mpatches.Patch(color=\"#1f77b4\", label=\"f(x)\"),\n",
    "        mpatches.Patch(color=\"#ff7f0e\", label=\"g(x)\")\n",
    "    ])\n",
    "\n",
    "    plt.scatter(X, y, c=\"#1f77b4\")\n",
    "    plt.plot(xlim, G(xlim, w), c=\"#ff7f0e\")\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lineare regression mit featuretransformation\n",
    "\n",
    "# featuretranformation (X -> Z)\n",
    "def phi(x, Q=4):\n",
    "    return np.array([x**i for i in range(1, Q+1)])\n",
    "\n",
    "def PHI(X, Q=4):\n",
    "    return np.concatenate([phi(x, Q).reshape(1, -1) for x in X])\n",
    "\n",
    "# g ~ f (x -> z; z, w -> y)\n",
    "def g(x, w, Q=4):\n",
    "    return np.hstack((np.ones(1), phi(x, Q))) @ w\n",
    "\n",
    "def G(X, w, Q=4):\n",
    "    return [g(x, w, Q) for x in X]\n",
    "\n",
    "# lineare regresseion (X -> Z; Z, y -> w)\n",
    "def lin_reg_featuretransformed(X, y, Q=4):\n",
    "    return lin_reg(PHI(X, Q), y)\n",
    "    \n",
    "def scatter(X, y, w, f=None, Q=4, start=None, stop=None, K=50):\n",
    "    if start is None or stop is None:\n",
    "        L = np.linspace(X.min(), X.max(), K)\n",
    "    else:\n",
    "        L = np.linspace(start, stop, K)\n",
    "\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"y\")\n",
    "\n",
    "    plt.legend(handles=[\n",
    "        mpatches.Patch(color=\"#1f77b4\", label=\"f(x)\"),\n",
    "        mpatches.Patch(color=\"#ff7f0e\", label=\"g(x)\")\n",
    "    ])\n",
    "\n",
    "    plt.scatter(X, y, c=\"#1f77b4\")\n",
    "    if f is not None: plt.plot(L, f(L), c=\"#1f77b4\")\n",
    "    plt.plot(L, G(L, w, Q), c=\"#ff7f0e\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lineare regression mit regularisierung\n",
    "\n",
    "# lineare regression (X, y -> w)\n",
    "def lin_reg_regularized(X, y, l=0):\n",
    "    X = np.column_stack((np.ones(X.shape[0]), X))\n",
    "\n",
    "    X_d = np.linalg.inv(X.T @ X + l * np.identity(np.min(X.shape))) @ X.T\n",
    "    return X_d @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lineare regression mit featuretransformation und regularisierung\n",
    "\n",
    "# lineare regression (X -> Z; Z, y -> w)\n",
    "def lin_reg_featuretransformed_regularized(X, y, Q=4, l=0):\n",
    "    return lin_reg_regularized(PHI(X, Q), l)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistische regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_in(X, y, w):\n",
    "    return np.sum([np.log(1 + np.exp(-y[i] * w @ x)) for i, x in enumerate(X)]) / X.shape[0]\n",
    "\n",
    "# phi (x -> [0, 1])\n",
    "def phi(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# gradient (X, y, w -> E_in)\n",
    "def gradient(X, y, w):\n",
    "    return np.sum([(-y[i]*x) * phi(-y[i] * w @ x) for i, x in enumerate(X)], axis=0) / X.shape[0]\n",
    "\n",
    "# gradientenabstieg (X, y, w_0 -> w)\n",
    "def grad_descent(X, y, w=None, n=0.1, T=400, eps_e=10**-8, eps_g=10**-8):\n",
    "    if w is None:\n",
    "        w = np.random.normal(0, 1, X.shape[1])\n",
    "\n",
    "    for _ in range(T):\n",
    "        g = gradient(X, y, w)\n",
    "        v = -g / np.linalg.norm(g)\n",
    "\n",
    "        w = w + n*v\n",
    "\n",
    "        if e_in(X, y, w) < eps_e: break\n",
    "        if np.linalg.norm(g) < eps_g: break\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# entscheidungsbÃ¤ume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropie\n",
    "def entropy(X):\n",
    "    def xlogx(x):\n",
    "        if x == 0: return 0\n",
    "        return x * np.log2(x)\n",
    "    \n",
    "    return -np.sum([xlogx(sum([x == l for l in X]) / len(X)) for x in np.unique(X)])\n",
    "\n",
    "# information gain\n",
    "def info_gain(y, x_j, z):\n",
    "    i = x_j < z\n",
    "    return entropy(y) - np.sum(i) / len(y) * entropy(y[i]) - np.sum(~i) / len(y) * entropy(y[~i])\n",
    "\n",
    "def max_info_gain(y, X):\n",
    "    IG = 0\n",
    "    ret = [None, None]\n",
    "\n",
    "    for j in range(X.shape[1]):\n",
    "        for z in np.unique(X[:,j]):\n",
    "            if info_gain(y, X[:,j], z) > IG:\n",
    "                IG = info_gain(y, X[:,j], z)\n",
    "                ret = [j, z]\n",
    "\n",
    "    return ret\n",
    "\n",
    "# entscheidungsbaum\n",
    "class DecisionTreeClassifier(object):\n",
    "    def __init__(self, max_depth=5):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y, depth=0):\n",
    "        def make_node(feature=None, split=None, left=None, right=None):\n",
    "            values, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "            return {\n",
    "                'depth': depth,\n",
    "                'feature': feature,\n",
    "                'split': split,\n",
    "                'value': values[np.argmax(counts)],\n",
    "                'left': left,\n",
    "                'right': right\n",
    "            }\n",
    "\n",
    "        if len(y) == 0: return None\n",
    "        if depth >= self.max_depth or len(np.unique(y)) == 1: return make_node()\n",
    "            \n",
    "        i, z = max_info_gain(y, X)\n",
    "        if z is None: return make_node()\n",
    "\n",
    "        idx = np.array([x[i] < z for x in X])\n",
    "        node = make_node(i, z, self.fit(X[idx], y[idx], depth+1), self.fit(X[~idx], y[~idx], depth+1))\n",
    "        \n",
    "        if depth == 0:\n",
    "            self.tree = node\n",
    "            return self\n",
    "            \n",
    "        return node\n",
    "            \n",
    "\n",
    "    def predict(self, X):\n",
    "        def rek_predict(x, ptr):\n",
    "            if ptr['split'] is None: return ptr['value']\n",
    "            if x[ptr['feature']] < ptr['split']: return rek_predict(x, ptr['left'])\n",
    "            return rek_predict(x, ptr['right'])\n",
    "            \n",
    "        return [rek_predict(x, self.tree) for x in X]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LS-SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_rbf(X, X_, sigma):\n",
    "    return np.exp(-cdist(X, X_, metric=\"euclidean\")**2 / (2 * sigma**2))\n",
    "\n",
    "def omega(X, y, sigma):\n",
    "    return (y @ y.T) * kernel_rbf(X, X, sigma)\n",
    "    \n",
    "def lssvm(X, y, sigma, gamma):\n",
    "    A = np.block([\n",
    "        [0, y.T],\n",
    "        [y, omega(X, y, sigma) + np.identity(X.shape[0]) / gamma]\n",
    "    ])\n",
    "    z = np.concatenate(([0], np.ones(X.shape[0])))\n",
    "    \n",
    "    s = (np.linalg.inv(A) @ z)\n",
    "\n",
    "    return s[1:], s[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bonus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*}\n",
    "& H(X) && = - \\sum_{k=1}^C p_k \\log_2(p_k) \\\\\n",
    "& H(Y|X) && = - \\sum_j p(x_j) \\left(\\sum_i p(y_i|x_j) \\log_2 \\left(p(y_i|x_j)\\right)\\right) \\\\\n",
    "& IG(Y,X) && = H(Y) - H(Y|X) \\\\\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropien und information gain\n",
    "def H(X, y=None, tex=False):\n",
    "    def xlogx(x):\n",
    "        if x == 0: return 0\n",
    "        return x * np.log2(x)\n",
    "    \n",
    "    if tex: print(\"= \", end=\"\")\n",
    "\n",
    "    if y is None:\n",
    "        h = 0\n",
    "        for x in set(X):\n",
    "            if tex:\n",
    "                frac_p_k = f\"\\\\frac{{{sum([x == l for l in X])}}}{{{len(X)}}}\"\n",
    "                print(f\"- {frac_p_k} \\\\log_2\\\\left({frac_p_k}\\\\right) \", end=\"\")\n",
    "\n",
    "            h -= xlogx(sum([x == l for l in X]) / len(X))\n",
    "    else:\n",
    "        h = 0\n",
    "        for x in set(X):\n",
    "            idx = [i for i, l in enumerate(X) if x == l]\n",
    "            p_yx = [y[i] for i in idx]\n",
    "            \n",
    "            if tex:\n",
    "                frac_p_x = f\"\\\\frac{{{len(idx)}}}{{{len(X)}}}\"\n",
    "                print(f\"- {frac_p_x} \\\\left(\", end=\"\")\n",
    "\n",
    "                frac_p_yx = f\"\\\\frac{{{sum(p_yx)}}}{{{len(p_yx)}}}\"\n",
    "                print(f\"{frac_p_yx} \\\\log_2\\\\left({frac_p_yx}\\\\right) \", end=\"\")\n",
    "                frac_p_yx = f\"\\\\frac{{{len(p_yx)-sum(p_yx)}}}{{{len(p_yx)}}}\"\n",
    "                print(f\"+ {frac_p_yx} \\\\log_2\\\\left({frac_p_yx}\\\\right) \", end=\"\")\n",
    "\n",
    "                print(f\"\\\\right) \", end=\"\")\n",
    "\n",
    "            p_yx = sum(p_yx) / len(p_yx)\n",
    "\n",
    "            h -= len(idx) / len(X) * (xlogx(p_yx) + xlogx(1 - p_yx))\n",
    "    return h\n",
    "\n",
    "def IG(X, y):\n",
    "    return H(y) - H(X, y)\n",
    "\n",
    "def ig_to_tex(X, y, labels=None):\n",
    "    print(\"$$\\\\begin{align*}\")\n",
    "    print(\"\\\\\\\\\")\n",
    "\n",
    "    print(f\"& H(X) && = - \\\\sum_{{k=1}}^C p_k \\\\log_2(p_k) & \\\\\\\\\")\n",
    "    print(f\"& H(Y|X) && = - \\\\sum_j p(x_j) \\\\left(\\\\sum_i p(y_i|x_j) \\\\log_2 \\\\left(p(y_i|x_j)\\\\right)\\\\right) & \\\\\\\\\")\n",
    "    print(f\"& IG(Y,X) && = H(Y) - H(Y|X) &  \\\\\\\\\")\n",
    "    print(\"\\\\\\\\\")\n",
    "\n",
    "    print(f\"& H(Y) = \\\\mathbb{{E}}[i] && \", end=\"\")\n",
    "    print(f\"& \\\\approx {H(y, tex=True):.5f} \\\\\\\\\")\n",
    "    print(\"\\\\\\\\\")\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        label = f\"\\\\text{{{labels[i]}}}\" if labels is not None else f\"\\\\text{{X[{i}]}}\"\n",
    "        \n",
    "        print(f\"& H(Y|{label}) && \", end=\"\")\n",
    "        print(f\"& \\\\approx {H(X[i], y, tex=True):.5f} \\\\\\\\\")\n",
    "        \n",
    "        print(f\"& IG(Y,{label}) && = H(Y) - H(Y|{label}) & \\\\approx {IG(X[i], y):.5f} \\\\\\\\\")\n",
    "        print(\"\\\\\\\\\")\n",
    "    \n",
    "    print(\"\\\\end{align*}$$\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2 (default, Feb 28 2021, 17:03:44) \n[GCC 10.2.1 20210110]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
